{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: jsonapi_client in /home/johannes/.local/lib/python3.10/site-packages (0.9.9)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from jsonapi_client) (2.25.1)\n",
      "Requirement already satisfied: aiohttp in /home/johannes/.local/lib/python3.10/site-packages (from jsonapi_client) (3.8.5)\n",
      "Requirement already satisfied: jsonschema in /home/johannes/.local/lib/python3.10/site-packages (from jsonapi_client) (4.19.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/johannes/.local/lib/python3.10/site-packages (from aiohttp->jsonapi_client) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/johannes/.local/lib/python3.10/site-packages (from aiohttp->jsonapi_client) (1.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/johannes/.local/lib/python3.10/site-packages (from aiohttp->jsonapi_client) (3.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/johannes/.local/lib/python3.10/site-packages (from aiohttp->jsonapi_client) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/johannes/.local/lib/python3.10/site-packages (from aiohttp->jsonapi_client) (1.9.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/johannes/.local/lib/python3.10/site-packages (from aiohttp->jsonapi_client) (6.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/johannes/.local/lib/python3.10/site-packages (from aiohttp->jsonapi_client) (1.3.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/johannes/.local/lib/python3.10/site-packages (from jsonschema->jsonapi_client) (0.10.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/johannes/.local/lib/python3.10/site-packages (from jsonschema->jsonapi_client) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/johannes/.local/lib/python3.10/site-packages (from jsonschema->jsonapi_client) (0.30.2)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/lib/python3/dist-packages (from yarl<2.0,>=1.0->aiohttp->jsonapi_client) (3.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install jsonapi_client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jsonapi_client import Session\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "###################\n",
    "# Definie Functions\n",
    "###################\n",
    "\n",
    "# retuns list of id`s for all Studies to Biom\n",
    "def GetBiomStudyIds(biom_id: str):\n",
    "    with Session(\"https://www.ebi.ac.uk/metagenomics/api/v1\") as mgnify:\n",
    "        \n",
    "        biomes_dfs = []\n",
    "        for r in mgnify.iterate(f'biomes/{biom_id}/studies'):\n",
    "            biom_df = pd.json_normalize(r.json)\n",
    "            biom_df['url'] = str(r.links.self)\n",
    "            biomes_dfs.append(biom_df)\n",
    "        \n",
    "    main_biomes_df = pd.concat(biomes_dfs)\n",
    "\n",
    "    main_biomes_df = main_biomes_df.dropna()\n",
    "\n",
    "    return main_biomes_df['id'].to_list()\n",
    "\n",
    "\n",
    "# returns List of analysis ids for list of studies\n",
    "def GetAnalysesIdsOfStudies(study_ids : list[str]):\n",
    "    biomes_dfs = []\n",
    "    for study in study_ids:\n",
    "        with Session(\"https://www.ebi.ac.uk/metagenomics/api/v1\") as mgnify:\n",
    "            for r in mgnify.iterate(f'studies/{study}/analyses'):\n",
    "                biom_df = pd.json_normalize(r.json)\n",
    "                biom_df['url'] = str(r.links.self)\n",
    "                biomes_dfs.append(biom_df)\n",
    "\n",
    "                # only take first studie\n",
    "\n",
    "                break\n",
    "        \n",
    "    biomes_dfs = pd.concat(biomes_dfs)['id'].to_list()\n",
    "    return biomes_dfs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'test/testForNorm/norm_test_1.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 102\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[39mreturn\u001b[39;00m normalized_df\n\u001b[1;32m    100\u001b[0m \u001b[39m# testing\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39mtest/testForNorm/norm_test_1.tsv\u001b[39;49m\u001b[39m'\u001b[39;49m,sep\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    104\u001b[0m df2 \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m    106\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatetime\u001b[39;00m \u001b[39mimport\u001b[39;00m datetime\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    579\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1662\u001b[0m     f,\n\u001b[1;32m   1663\u001b[0m     mode,\n\u001b[1;32m   1664\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1665\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1666\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1667\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1668\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1669\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1670\u001b[0m )\n\u001b[1;32m   1671\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    860\u001b[0m             handle,\n\u001b[1;32m    861\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    862\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    863\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    864\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    865\u001b[0m         )\n\u001b[1;32m    866\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test/testForNorm/norm_test_1.tsv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#########################\n",
    "# normalization functions\n",
    "#########################\n",
    "\n",
    "\n",
    "# norms in tool: vst, rlog, tmm, RLE, Upperquartile\n",
    "\n",
    "# => look if possible to implement / else: try to implement  with tool and use other normalisation here.\n",
    "\n",
    "# maybe try  Min-Max scaling, Z-score normalization, decimal scaling, and log transformation\n",
    "\n",
    "#df = pd.read_csv('outputs/collection/(root:Environmental:Aquatic:Marine)MGYS00006028.tsv',sep='\\t')\n",
    "\n",
    "#returns normalized df\n",
    "def NormalizeDf(_inputDF, method):\n",
    "    \n",
    "    if(method == \"softmax\"):\n",
    "        return SoftmaxNormalization(_inputDF)\n",
    "    elif(method == \"standart\"):\n",
    "        return StandartNormalization(_inputDF)\n",
    "    elif(method == \"log\"):\n",
    "        return LogNormalization(_inputDF)\n",
    "    elif(method == \"log10\"):\n",
    "        return Log10Normalization(_inputDF)\n",
    "    elif(method == \"sigmoid\"):\n",
    "        return SigmoidNormalization(_inputDF)\n",
    "    elif(method == \"quantile\"):\n",
    "        return quantile_normalize(_inputDF)\n",
    "    else:\n",
    "        print(f\"no normalization method found with the name {method}\")\n",
    "        return _inputDF # return unput df to prevent crashes\n",
    "\n",
    "def StandartNormalization(_inputDF):\n",
    "    for col in _inputDF.columns.tolist():\n",
    "        if(col in ['superkingdom','kingdom','phylum']):\n",
    "            continue\n",
    "        _divider = sum(_inputDF[col])\n",
    "        _inputDF[col] = _inputDF[col] / _divider\n",
    "    return _inputDF\n",
    "        ### normalization part\n",
    "\n",
    "def SoftmaxNormalization(_inputDF):\n",
    "    _inputDF = StandartNormalization(_inputDF)\n",
    "    _inputDF.replace(0.0,np.nan)\n",
    "    numeric_columns = _inputDF.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "    for col in numeric_columns:\n",
    "        _inputDF[col] = np.exp(_inputDF[col]) / np.sum(np.exp(_inputDF[col]))\n",
    "        pass\n",
    "\n",
    "    return _inputDF\n",
    "\n",
    "\n",
    "def LogNormalization(_inputDF):\n",
    "    numeric_columns = _inputDF.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_columns:\n",
    "        _inputDF[col] = _inputDF[col].apply(np.log)\n",
    "    return _inputDF.replace(-np.inf, 0)\n",
    "\n",
    "def Log10Normalization(_inputDF):\n",
    "    numeric_columns = _inputDF.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_columns:\n",
    "        _inputDF[col] = _inputDF[col].apply(np.log10)\n",
    "    return _inputDF.replace(-np.inf, 0)\n",
    "\n",
    "def SigmoidNormalization(_inputDF):\n",
    "    _inputDF = StandartNormalization(_inputDF)\n",
    "    def sigmoid(x):\n",
    "        return ((1 / (1 + np.exp(-x))) - .5) * 2    # -.5 and time to so we get values between 0 and 1\n",
    "    _inputDF = _inputDF.apply(lambda x: sigmoid(x) if np.issubdtype(x.dtype, np.number) else x)\n",
    "\n",
    "    return _inputDF\n",
    "\n",
    "\n",
    "# _inputDF.apply(lambda x: (((1 / (1 + np.exp(-x))) - .5) * 2)  if np.issubdtype(x.dtype, np.number) else x)\n",
    "\n",
    "# ??\n",
    "def quantile_normalize(df):\n",
    "    normalized_df = df.copy()\n",
    "    numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "    ranks = df[numeric_columns].rank(method='average')\n",
    "    mean_ranks = ranks.mean(axis=1)\n",
    "    \n",
    "    sorted_ranks = mean_ranks.sort_values()\n",
    "    quantiles = sorted_ranks / len(sorted_ranks)\n",
    "    #return quantiles\n",
    "    for column in numeric_columns:\n",
    "        if 0 in df[column].values:\n",
    "            df[column].replace(0, np.nan, inplace=True)\n",
    "            sorted_column = df[column].sort_values()\n",
    "            normalized_df[column] = np.interp(df[column], sorted_column, quantiles)\n",
    "            normalized_df[column].fillna(0, inplace=True)\n",
    "        else:\n",
    "            sorted_column = df[column].sort_values()\n",
    "            normalized_df[column] = np.interp(df[column], sorted_column, quantiles)\n",
    "\n",
    "    return normalized_df\n",
    "# testing\n",
    "\"\"\"\n",
    "df = pd.read_csv('test/testForNorm/norm_test_1.tsv',sep='\\t')\n",
    "\n",
    "df2 = df.copy()\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "t1 = datetime.now()\n",
    "\n",
    "df = quantile_normalize(df)\n",
    "\"\"\"\n",
    "#df.to_csv(\"test_soft_1.tsv\",sep='\\t')\n",
    "\n",
    "#t2 = datetime.now()\n",
    "\n",
    "#df2 = quantile_normalize(df2)\n",
    "\n",
    "#df2.to_csv(\"test_soft_2.tsv\",sep='\\t')\n",
    "\n",
    "#t3 = datetime.now()\n",
    "\n",
    "#print(f'{(t2-t1).total_seconds()},{(t3-t2).total_seconds()}')\n",
    "\n",
    "#df.to_csv('outputs/collection/root:Environmental:Terrestrial:Soil_20_2_softmax.tsv',sep='\\t')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment '.venv' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "from jsonapi_client import Session\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# root:Environmental:Aquatic:Marine\n",
    "#\"root:Environmental:Terrestrial:Soil\"\n",
    "biom_ids = [\"root:Environmental:Terrestrial:Soil\",\"root:Environmental:Aquatic:Marine\"]\n",
    "\n",
    "normalization_methods = ['softmax','log','sigmoid','quantile']\n",
    "\n",
    "phylumDepths = [1,2]\n",
    "\n",
    "max_studies = 20\n",
    "\n",
    "for biom_id in biom_ids:\n",
    "\n",
    "    #########################################\n",
    "    # Get All Studies for Biom\n",
    "    #########################################\n",
    "\n",
    "    study_ids = GetBiomStudyIds(biom_id)\n",
    "    print(f'found {len(study_ids)} studies for {biom_id}')\n",
    "    #########################################\n",
    "    # Create outputFolder\n",
    "    #########################################\n",
    "\n",
    "    data_tsvs = []\n",
    "\n",
    "\n",
    "    #########################################\n",
    "    # remove Studies IDS wich result in dying Kernel\n",
    "\n",
    "    # maybe only use ver 5.0 for smaller files?\n",
    "    #########################################\n",
    "    \n",
    "    remove_ids = [\"MGYS00003194\"]\n",
    "    for id in remove_ids:\n",
    "        if(id in study_ids):\n",
    "            study_ids.remove(id)\n",
    "    \n",
    "\n",
    "    data_output_folder = 'outputs/collection'\n",
    "    os.makedirs(data_output_folder, exist_ok=True)\n",
    "    \"\"\"\n",
    "    if(len(study_ids) > max_studies):\n",
    "        study_ids = study_ids[:max_studies]\n",
    "    \"\"\"\n",
    "    found_studies = 0\n",
    "\n",
    "    for study_id in study_ids:\n",
    "        if(found_studies == max_studies):\n",
    "            break\n",
    "        print(f\"start {study_id}\")\n",
    "        #########################################\n",
    "        # Get all downloads for one study\n",
    "        #########################################\n",
    "        with Session(\"https://www.ebi.ac.uk/metagenomics/api/v1\") as mgnify:\n",
    "            \n",
    "            dfs = []\n",
    "            for r in mgnify.iterate(f'studies/{study_id}/downloads'):\n",
    "                df = pd.json_normalize(r.json)\n",
    "                df['url'] = str(r.links.self)\n",
    "                dfs.append(df)\n",
    "\n",
    "        # if no data for studie is found / studie has no data\n",
    "        if(dfs == []):\n",
    "            continue\n",
    "        main_df = pd.concat(dfs)\n",
    "\n",
    "        # print(main_df)\n",
    "        \n",
    "        ##########################\n",
    "        # get specific data url\n",
    "        #########################\n",
    "\n",
    "\n",
    "\n",
    "        data_type = \"TSV\"\n",
    "        data_label = \"Phylum level taxonomies SSU (TSV)\"#\"Phylum level taxonomies LSU (TSV)\"\n",
    "        pipeline_version = \"5.0\"\n",
    "\n",
    "\n",
    "        c1 = main_df[\"attributes.file-format.name\"] == data_type\n",
    "        c2 = main_df[\"attributes.description.description\"] == data_label\n",
    "        c3 = main_df[\"relationships.pipeline.data.id\"] == pipeline_version\n",
    "\n",
    "        url_df = main_df.loc[(c1 & c2 & c3), \"url\"] #just added pipeline version\n",
    "        if(len(url_df.index) > 0):\n",
    "            url = url_df.iloc[0]\n",
    "        else:\n",
    "            print(f\"Found no {data_type} with description {data_label} for {study_id}\")\n",
    "            continue\n",
    "\n",
    "        ##########################\n",
    "        # download the data\n",
    "        #########################\n",
    "\n",
    "        response = requests.get(url)\n",
    "\n",
    "        if not response:\n",
    "            print(f\"Could not download file for {study_id}, got response: {response.status_code}\")\n",
    "            continue # break brakes whole loop\n",
    "\n",
    "        found_studies += 1\n",
    "\n",
    "        print(url)\n",
    "\n",
    "        data_output_path = os.path.join(data_output_folder, f\"({biom_id}){study_id}.tsv\")\n",
    "        with open(data_output_path, \"w\") as f:\n",
    "            f.write(response.text)\n",
    "\n",
    "        data_tsv = pd.read_table(data_output_path, sep=\"\\t\")\n",
    "\n",
    "        # open tsv header\n",
    "        #header_row = data_tsv.iloc[0,:].tolist()\n",
    "        if(\"superkingdom\" not in data_tsv.columns):\n",
    "            data_tsv.insert(0,\"superkingdom\",'Unassigned')\n",
    "        data_tsv = data_tsv.set_index(['superkingdom','kingdom','phylum'])\n",
    "\n",
    "        data_tsvs.append(data_tsv)\n",
    "\n",
    "    print(f'found {found_studies} for {biom_id}')\n",
    "\n",
    "    total_tsv = pd.concat(data_tsvs,axis=1)\n",
    "    total_tsv = total_tsv.fillna(0)\n",
    "\n",
    "    #drop unassinged at pylum level for normalization\n",
    "    if('phylum' in total_tsv.columns):\n",
    "        total_tsv.drop(total_tsv[total_tsv['phylum'] == 'Unassigned'].index, inplace = True)\n",
    "\n",
    "    #save first go\n",
    "    data_output_path = os.path.join(data_output_folder, f\"{biom_id}_{found_studies}_-1_none.tsv\")\n",
    "\n",
    "    total_tsv.to_csv(data_output_path, sep=\"\\t\")\n",
    "\n",
    "\n",
    "\n",
    "    # read without index so we can sum easely\n",
    "    total_tsv = pd.read_table(data_output_path, sep=\"\\t\")\n",
    "\n",
    "    clean_output_path = data_output_path\n",
    "\n",
    "\n",
    "    for normalization_method in normalization_methods:\n",
    "        base_total_tsv = pd.read_table(clean_output_path,sep='\\t',index_col=1)                  # load tsv with no norm\n",
    "        print(f'start {normalization_method}')\n",
    "        base_total_tsv = NormalizeDf(base_total_tsv,normalization_method)\n",
    "        data_output_path = os.path.join(data_output_folder, f\"{biom_id}_{found_studies}_-1_{normalization_method}.tsv\")\n",
    "        base_total_tsv.to_csv(data_output_path, sep=\"\\t\")                           # save as normed tsv\n",
    "\n",
    "    ###############################\n",
    "    # colapse rows to specifig depth (depth on phylum level (ignoring Kingdom and Superkingdom))\n",
    "    ###############################\n",
    "    for index_depth in phylumDepths:\n",
    "        index_seperator = '_'\n",
    "        # Load tsv with no norm\n",
    "        total_tsv = pd.read_table(clean_output_path, sep=\"\\t\")\n",
    "\n",
    "        index_row = total_tsv.iloc[:,2].tolist()\n",
    "\n",
    "        for i in range(len(index_row)):\n",
    "            index = index_row[i].split(index_seperator)\n",
    "            current_index = []\n",
    "            if(len(index) < index_depth):\n",
    "                current_index = index\n",
    "            else:\n",
    "                current_index = index[:index_depth]\n",
    "            index_row[i] = index_seperator.join(current_index)\n",
    "        \n",
    "        total_tsv.iloc[:,2] = index_row\n",
    "\n",
    "        total_tsv = total_tsv.groupby([\"superkingdom\",\"kingdom\",\"phylum\"]).sum()\n",
    "\n",
    "        # drop unassinged in phylumn\n",
    "\n",
    "\n",
    "        \n",
    "        # save table of phylum level\n",
    "        data_output_path = os.path.join(data_output_folder, f\"{biom_id}_{found_studies}_{index_depth}_none.tsv\")\n",
    "        total_tsv.to_csv(data_output_path, sep=\"\\t\")\n",
    "\n",
    "\n",
    "        clean_output_path_phylum = data_output_path\n",
    "        # normalize table\n",
    "\n",
    "        for normalization_method in normalization_methods:\n",
    "            total_tsv = pd.read_table(clean_output_path_phylum,sep='\\t',index_col=1)\n",
    "            print(f'start {normalization_method}')\n",
    "            total_tsv = NormalizeDf(total_tsv,normalization_method)\n",
    "            data_output_path = os.path.join(data_output_folder, f\"{biom_id}_{found_studies}_{index_depth}_{normalization_method}.tsv\")\n",
    "            total_tsv.to_csv(data_output_path, sep=\"\\t\")\n",
    "\n",
    "        # add to galaxy (manually)\n",
    "        # put(data_output_path)             #remove coment for Galaxy (used to export from notebook to galaxy)\n",
    "\n",
    "main_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
