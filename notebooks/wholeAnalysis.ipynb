{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: jsonapi_client in /home/johannes/.local/lib/python3.10/site-packages (0.9.9)\n",
      "Requirement already satisfied: aiohttp in /home/johannes/.local/lib/python3.10/site-packages (from jsonapi_client) (3.9.0)\n",
      "Requirement already satisfied: jsonschema in /home/johannes/.local/lib/python3.10/site-packages (from jsonapi_client) (4.20.0)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from jsonapi_client) (2.25.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/johannes/.local/lib/python3.10/site-packages (from aiohttp->jsonapi_client) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/johannes/.local/lib/python3.10/site-packages (from aiohttp->jsonapi_client) (23.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/johannes/.local/lib/python3.10/site-packages (from aiohttp->jsonapi_client) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/johannes/.local/lib/python3.10/site-packages (from aiohttp->jsonapi_client) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/johannes/.local/lib/python3.10/site-packages (from aiohttp->jsonapi_client) (1.9.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/johannes/.local/lib/python3.10/site-packages (from aiohttp->jsonapi_client) (6.0.4)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/johannes/.local/lib/python3.10/site-packages (from jsonschema->jsonapi_client) (2023.11.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/johannes/.local/lib/python3.10/site-packages (from jsonschema->jsonapi_client) (0.13.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/johannes/.local/lib/python3.10/site-packages (from jsonschema->jsonapi_client) (0.31.0)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/lib/python3/dist-packages (from yarl<2.0,>=1.0->aiohttp->jsonapi_client) (3.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install jsonapi_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputBioms = [\"root:Host-associated:Human:Digestive system:Oral:Subgingival plaque\",\"root:Host-associated:Human:Digestive system:Oral:Supragingival plaque\"]\n",
    "\n",
    "runName = \"run_01\"\n",
    "\n",
    "max_sample_count = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jsonapi_client import Session\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "\n",
    "###################\n",
    "# Definie Functions\n",
    "###################\n",
    "\n",
    "########################################################\n",
    "# input: biom string from MGnify\n",
    "# https://www.ebi.ac.uk/metagenomics/browse/biomes/\n",
    "# example: GetBiomSamplesByIds(root:Engineered:Bioreactor)\n",
    "#\n",
    "#\n",
    "# output: df with the first <max_sample_count> \n",
    "########################################################\n",
    "def GetBiomSamplesByIds(biom_id: str):\n",
    "    with Session(\"https://www.ebi.ac.uk/metagenomics/api/v1\") as mgnify:\n",
    "        i = 0\n",
    "        biomes_dfs = []\n",
    "        for r in mgnify.iterate(f'biomes/{biom_id}/samples'):\n",
    "            biom_df = pd.json_normalize(r.json)\n",
    "            biom_df['url'] = str(r.links.self)\n",
    "            biomes_dfs.append(biom_df)\n",
    "            i += 1\n",
    "            if(i == max_sample_count):\n",
    "                break\n",
    "    main_biomes_df = pd.concat(biomes_dfs)\n",
    "    return main_biomes_df\n",
    "\n",
    "########################################################\n",
    "# input: study ACCESSION from mgnify\n",
    "# https://www.ebi.ac.uk/metagenomics/browse/studies\n",
    "# example: GetBiomSamplesByIds(MGYS00006539)\n",
    "#\n",
    "#\n",
    "# output: list with 2 objects\n",
    "# when output[0] == None, we have a study without any analysis data\n",
    "# when output[0] == 1,  output[1] = df with all meta-analysis Informations (accession, date, version, ...) for each sample in the study\n",
    "########################################################\n",
    "\n",
    "def GetAnalysisFromStudy(_study_id):\n",
    "    with Session(\"https://www.ebi.ac.uk/metagenomics/api/v1\") as mgnify:\n",
    "        biomes_dfs = []\n",
    "        for r in mgnify.iterate(f'studies/{_study_id}/analyses'):\n",
    "            biom_df = pd.json_normalize(r.json)\n",
    "            biom_df['url'] = str(r.links.self)\n",
    "            biomes_dfs.append(biom_df)\n",
    "    # testing if study has no analysis data\n",
    "    if(biomes_dfs == []):\n",
    "        return [None,None]\n",
    "    main_biomes_df = pd.concat(biomes_dfs)\n",
    "\n",
    "    return [1,main_biomes_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start grab for MGYA00661793\n",
      "https://www.ebi.ac.uk/metagenomics/api/v1/analyses/MGYA00661793/file/ERZ14207391_FASTA_SSU_OTU.tsv\n",
      "start grab for MGYA00661792\n",
      "https://www.ebi.ac.uk/metagenomics/api/v1/analyses/MGYA00661792/file/ERZ14207393_FASTA_SSU_OTU.tsv\n",
      "start grab for MGYA00661791\n",
      "https://www.ebi.ac.uk/metagenomics/api/v1/analyses/MGYA00661791/file/ERZ14207392_FASTA_SSU_OTU.tsv\n",
      "start grab for MGYA00661789\n",
      "https://www.ebi.ac.uk/metagenomics/api/v1/analyses/MGYA00661789/file/ERZ14207394_FASTA_SSU_OTU.tsv\n",
      "start grab for MGYA00661787\n",
      "https://www.ebi.ac.uk/metagenomics/api/v1/analyses/MGYA00661787/file/ERZ14207366_FASTA_SSU_OTU.tsv\n",
      "start grab for MGYA00661785\n",
      "https://www.ebi.ac.uk/metagenomics/api/v1/analyses/MGYA00661785/file/ERZ14207367_FASTA_SSU_OTU.tsv\n",
      "start grab for MGYA00661784\n",
      "https://www.ebi.ac.uk/metagenomics/api/v1/analyses/MGYA00661784/file/ERZ14207368_FASTA_SSU_OTU.tsv\n",
      "start grab for MGYA00661783\n",
      "https://www.ebi.ac.uk/metagenomics/api/v1/analyses/MGYA00661783/file/ERZ14207369_FASTA_SSU_OTU.tsv\n",
      "start grab for MGYA00661782\n",
      "https://www.ebi.ac.uk/metagenomics/api/v1/analyses/MGYA00661782/file/ERZ14207372_FASTA_SSU_OTU.tsv\n",
      "start grab for MGYA00661780\n",
      "https://www.ebi.ac.uk/metagenomics/api/v1/analyses/MGYA00661780/file/ERZ14207373_FASTA_SSU_OTU.tsv\n",
      "start grab for MGYA00185690\n",
      "https://www.ebi.ac.uk/metagenomics/api/v1/analyses/MGYA00185690/file/ERR2624743_MERGED_FASTQ_SSU_OTU.tsv\n",
      "start grab for MGYA00185707\n",
      "https://www.ebi.ac.uk/metagenomics/api/v1/analyses/MGYA00185707/file/ERR2624744_MERGED_FASTQ_SSU_OTU.tsv\n",
      "start grab for MGYA00185672\n",
      "https://www.ebi.ac.uk/metagenomics/api/v1/analyses/MGYA00185672/file/ERR2624745_MERGED_FASTQ_SSU_OTU.tsv\n",
      "start grab for MGYA00185679\n",
      "https://www.ebi.ac.uk/metagenomics/api/v1/analyses/MGYA00185679/file/ERR2624746_MERGED_FASTQ_SSU_OTU.tsv\n",
      "start grab for MGYA00185703\n",
      "https://www.ebi.ac.uk/metagenomics/api/v1/analyses/MGYA00185703/file/ERR2624747_MERGED_FASTQ_SSU_OTU.tsv\n",
      "start grab for MGYA00185675\n",
      "https://www.ebi.ac.uk/metagenomics/api/v1/analyses/MGYA00185675/file/ERR2624748_MERGED_FASTQ_SSU_OTU.tsv\n",
      "start grab for MGYA00185710\n",
      "https://www.ebi.ac.uk/metagenomics/api/v1/analyses/MGYA00185710/file/ERR2624749_MERGED_FASTQ_SSU_OTU.tsv\n",
      "start grab for MGYA00185669\n",
      "https://www.ebi.ac.uk/metagenomics/api/v1/analyses/MGYA00185669/file/ERR2624750_MERGED_FASTQ_SSU_OTU.tsv\n",
      "start grab for MGYA00185695\n",
      "https://www.ebi.ac.uk/metagenomics/api/v1/analyses/MGYA00185695/file/ERR2624751_MERGED_FASTQ_SSU_OTU.tsv\n",
      "start grab for MGYA00185692\n",
      "https://www.ebi.ac.uk/metagenomics/api/v1/analyses/MGYA00185692/file/ERR2624752_MERGED_FASTQ_SSU_OTU.tsv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for biom in inputBioms:\n",
    "    df_samples = GetBiomSamplesByIds(biom)\n",
    "\n",
    "    sample_accessions = (df_samples['attributes.accession'].to_list())\n",
    "\n",
    "    studies = df_samples['relationships.studies.data'].to_list()\n",
    "    ####################\n",
    "    # for each of the studies we test if they have analysis data.\n",
    "    # if so we save the data in <study_analaysises>,\n",
    "    # until we have data for every sample.\n",
    "    ####################\n",
    "\n",
    "    # list of df with analysis data from studys\n",
    "    study_analaysises = []\n",
    "    # study-accessions from studys without analysis\n",
    "    studies_without_analysis = []\n",
    "    # study-accessions from studys having analysis data in <study_analaysises>\n",
    "    studies_Ids = []\n",
    "\n",
    "    for study in studies:\n",
    "        # list of studies for sample\n",
    "        s_ids_sample = []\n",
    "        for s in str(study).split(\"'\"):\n",
    "            if('MGYS' in s):\n",
    "                s_ids_sample.append(s) \n",
    "        # for each study\n",
    "        for st in s_ids_sample:\n",
    "            # test if we havent checked study\n",
    "            if(st in studies_Ids):\n",
    "                # we allready have analysis data for this study\n",
    "                break\n",
    "            if(st not in studies_without_analysis):\n",
    "                # Get analysis data for this study\n",
    "                study_analaysis = GetAnalysisFromStudy(st)\n",
    "                if(study_analaysis[0] == None):\n",
    "                    # discard if study has no analysis\n",
    "                    studies_without_analysis.append(st)\n",
    "                    continue\n",
    "                # keep if study has analysis\n",
    "                studies_Ids.append(st)\n",
    "                study_analaysises.append(study_analaysis[1])\n",
    "                break\n",
    "    # Merge all analysis data into one large df\n",
    "    study_analaysis = pd.concat(study_analaysises)\n",
    "    analysis_accesions = []\n",
    "    for sample_acc in sample_accessions:\n",
    "        acc = study_analaysis[study_analaysis['relationships.sample.data.id'] == sample_acc]['attributes.accession'].to_list()\n",
    "        analysis_accesions.append([len(acc),acc])\n",
    "    analysis_accesions\n",
    "\n",
    "    ##################################\n",
    "    # for each of of our samples we try to get the taxonomic assingments from MGnify\n",
    "    # if none of the analysis accessions of a sample return a file, we skip the sample\n",
    "    # (this happens when for e.g. the version of the MGnify pipeline is to old)\n",
    "    #\n",
    "    # Trying for \"OTUs and taxonomic assignments for SSU rRNA\" and \"All reads encoding SSU rRNA\" beacuse of different pipeline versions\n",
    "    #\n",
    "    # all Analysis files are saved in 'outputs/collection', so that they are automatically exported into Galaxy as one Collection\n",
    "    #\n",
    "    # analysis files are saved under 'outputs/collection/<analysis-accession>-<sample-accession>.tsv'\n",
    "    ##################################\n",
    "\n",
    "    # for each of our samples\n",
    "    for analysis_sample in analysis_accesions:\n",
    "        url = \"\"\n",
    "        # for each analysis-accession for the sample\n",
    "        for analysis_accession in analysis_sample[1]:\n",
    "            print(f\"start grab for {analysis_accession}\")\n",
    "            \n",
    "            # Get all downloads for one analysis\n",
    "            with Session(\"https://www.ebi.ac.uk/metagenomics/api/v1\") as mgnify:\n",
    "                \n",
    "                dfs = []\n",
    "                for r in mgnify.iterate(f'analyses/{analysis_accession}/downloads'):\n",
    "                    df = pd.json_normalize(r.json)\n",
    "                    df['url'] = str(r.links.self)\n",
    "                    dfs.append(df)\n",
    "                \n",
    "            main_df = pd.concat(dfs)\n",
    "\n",
    "            data_type = \"TSV\"\n",
    "            data_label = \"OTUs and taxonomic assignments for SSU rRNA\"\n",
    "            \n",
    "            # get URL for data_label = \"OTUs and taxonomic assignments for SSU rRNA\"\n",
    "            c1 = main_df[\"attributes.file-format.name\"] == data_type\n",
    "            c2 = main_df[\"attributes.description.description\"] == data_label\n",
    "            if(main_df.loc[(c1 & c2), \"url\"].size == 0):\n",
    "                # if we dont get a match try the same with data_label = 'All reads encoding SSU rRNA'\n",
    "                data_label = \"All reads encoding SSU rRNA\"\n",
    "                c2 = main_df[\"attributes.description.description\"] == data_label\n",
    "                if(main_df.loc[(c1 & c2), \"url\"].size == 0):\n",
    "                    continue\n",
    "                else:\n",
    "                    url = main_df.loc[(c1 & c2), \"url\"].iloc[0]\n",
    "                    break\n",
    "            url = main_df.loc[(c1 & c2), \"url\"].iloc[0]\n",
    "            break\n",
    "\n",
    "        if(url == \"\"):\n",
    "            print(f'no analysis found for {analysis_sample}')\n",
    "            continue\n",
    "\n",
    "        ##########################\n",
    "        # download the data\n",
    "        #########################\n",
    "\n",
    "        data_output_folder = f'outputs/collection/{runName}/{biom}'\n",
    "        os.makedirs(data_output_folder, exist_ok=True)\n",
    "\n",
    "        response = requests.get(url)\n",
    "\n",
    "        if not response:\n",
    "            print(f\"Could not download file, got response: {response.status_code}\")\n",
    "            break\n",
    "            \n",
    "        print(url)\n",
    "        data_output_path = os.path.join(data_output_folder, f\"{analysis_accession}-{study_analaysis[study_analaysis['attributes.accession'] == analysis_accession]['relationships.sample.data.id'].to_list()[0]}.tsv\")\n",
    "        with open(data_output_path, \"w\") as f:\n",
    "            f.write(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "def MergeOnTaxaLevelFunction(taxa, runName,biomName):\n",
    "    if(not os.path.isdir(f\"./outputs/collection/{runName}/merged/\")):\n",
    "        os.mkdir(f\"./outputs/collection/{runName}/merged/\")\n",
    "    galaxyInput_rankToMerge = taxa\n",
    "    # selection from \"superkingdom\" \"kingdom\" \"phylum\" \"class\" \"order\" \"family\" \"genus\" \"species\" \"all\" \"only counts\"\n",
    "\n",
    "    galaxyInput_countTables = runName\n",
    "\n",
    "\n",
    "    mappingFromTaxaToNumber = {\n",
    "        \"superkingdom\":1,\n",
    "        \"kingdom\":2,\n",
    "        \"phylum\":3,\n",
    "        \"class\":4,\n",
    "        \"order\":5,\n",
    "        \"family\":6,\n",
    "        \"genus\":7,\n",
    "        \"species\":8,\n",
    "        \"all\" : 10,         # to prevent key error in converting rank to number\n",
    "        \"only counts\" : 10  # only counts and all will be handled when creating the tables\n",
    "    }\n",
    "\n",
    "    rankToMerge = mappingFromTaxaToNumber[galaxyInput_rankToMerge]\n",
    "\n",
    "    filePaths = os.listdir(f\"./outputs/collection/{runName}/{biomName}\")\n",
    "\n",
    "    #filePaths = []\n",
    "\n",
    "    \n",
    "\n",
    "    tables = []\n",
    "    for file in filePaths:\n",
    "        # Read out all Tables and remove unneccecary informations\n",
    "        df = pd.read_csv(f\"./outputs/collection/{runName}/{biomName}/{file}\",sep=\"\\t\",header=1)\n",
    "        df = df.iloc[: , 1:]\n",
    "        if(\"taxid\" in df.columns):\n",
    "            df = df.drop(columns=[\"taxid\"])\n",
    "        df.columns = [file.split(\"/\")[-1].replace(\".tsv\",\"\"),\"#KEY\"]\n",
    "        columns = df.columns.tolist()\n",
    "        columns.reverse()\n",
    "        df = df[columns]\n",
    "        tables.append(df)\n",
    "    # Merge all tables into one\n",
    "    df = reduce(lambda df1,df2:pd.merge(df1,df2,on=\"#KEY\",how=\"outer\"),tables)\n",
    "    df = df.fillna(0)\n",
    "\n",
    "    if(galaxyInput_rankToMerge == \"only counts\"):\n",
    "        df.to_csv(f\"./outputs/collection/{runName}/merged/countTable_{biomName}.tsv\",sep=\"\\t\",index=False)\n",
    "\n",
    "    if(rankToMerge < 10):\n",
    "        df2 = df\n",
    "        df2 = df2.reset_index()\n",
    "        for index, row in df.iterrows():\n",
    "            rowRaw = row['#KEY']\n",
    "            rowSplit = rowRaw.split(\";\")\n",
    "            if(len(rowSplit) >= rankToMerge):\n",
    "                df2.loc[index,\"#KEY\"] = rowSplit[rankToMerge-1].replace(\"__\",\"_taxa_\")\n",
    "            else:\n",
    "                df2.loc[index,\"#KEY\"] = \"noValue\"\n",
    "\n",
    "        df2 = df2[df2[\"#KEY\"] != \"noValue\"]\n",
    "        df2 = df2.groupby([\"#KEY\"]).sum()\n",
    "        df2 = df2.reset_index()\n",
    "        df2 = df2.drop(columns=[\"index\"])\n",
    "        df2.to_csv(f\"./outputs/collection/{runName}/merged/mergedOn_{galaxyInput_rankToMerge}_{biomName}.tsv\",sep=\"\\t\",index=False)\n",
    "\n",
    "    if(galaxyInput_rankToMerge == \"all\"):\n",
    "        tables = []\n",
    "        for i in range(2,9):\n",
    "            df2 = df\n",
    "            df2 = df2.reset_index()\n",
    "            for index, row in df.iterrows():\n",
    "                rowRaw = row['#KEY']\n",
    "                rowSplit = rowRaw.split(\";\")\n",
    "                if(len(rowSplit) >= rankToMerge):\n",
    "                    df2.loc[index,\"#KEY\"] = rowSplit[i-1].replace(\"__\",\"_taxa_\")\n",
    "                else:\n",
    "                    df2.loc[index,\"#KEY\"] = \"noValue\"\n",
    "\n",
    "            df2 = df2[df2[\"#KEY\"] != \"noValue\"]\n",
    "            df2 = df2.groupby([\"#KEY\"]).sum()\n",
    "            df2 = df2.reset_index()\n",
    "            df2 = df2.drop(columns=[\"index\"])\n",
    "            tables.append(df2)\n",
    "        result = pd.concat(tables)\n",
    "        result.to_csv(f\"./outputs/collection/{runName}/merged/mergedOn_{galaxyInput_rankToMerge}_{biomName}.tsv\",sep=\"\\t\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root:Host-associated:Human:Digestive system:Oral:Subgingival plaque\n",
      "root:Host-associated:Human:Digestive system:Oral:Supragingival plaque\n"
     ]
    }
   ],
   "source": [
    "countFoldersForRun = os.listdir(f\"./outputs/collection/{runName}\")\n",
    "for folder in inputBioms:\n",
    "    print(folder)\n",
    "    for taxa in [\"superkingdom\",\"kingdom\",\"phylum\",\"class\",\"order\",\"family\",\"genus\",\"species\",\"all\",\"only counts\"]:\n",
    "        MergeOnTaxaLevelFunction(taxa,runName,folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
